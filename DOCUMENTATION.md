# VWL Multi-Agent RL - Entwicklungsdokumentation

Dieses Dokument beschreibt **WAS** gemacht wurde und **WARUM** bestimmte Entscheidungen getroffen wurden.

---

## Architektur-Entscheidungen

### Warum kein Staat am Anfang?

**Entscheidung:** Das Projekt startet NUR mit Haushalten und Unternehmen.

**Begr√ºndung:**
- Fokus auf Basis-Interaktion: Haushalte ‚Üî Unternehmen
- Einfacheres Environment f√ºr erste Tests
- Wirtschaftskreislauf ist bereits mit 2 Akteuren komplett
- Staat kann sp√§ter als 3. Agent hinzugef√ºgt werden

**Status:** ‚úÖ Implementiert in `envs/rllib_economy_env.py`

---

### Startbedingungen: Zuf√§llig pro Episode

**Entscheidung:** Startbedingungen werden **bei jedem Episode-Reset neu gezogen**.

**Was hei√üt das konkret?**
```python
# Bei jedem env.reset():
Episode 1: Haushalt_0 startet mit 2500‚Ç¨ (zuf√§llig aus [1000-5000‚Ç¨])
Episode 2: Haushalt_0 startet mit 4100‚Ç¨ (neu gezogen!)
Episode 3: Haushalt_0 startet mit 1800‚Ç¨ (neu gezogen!)
```

**Begr√ºndung:**
1. **Robustere Policy:** Agent lernt mit verschiedenen Startbedingungen umzugehen
2. **Generalisierung:** Policy funktioniert f√ºr arme UND reiche Haushalte
3. **Realit√§tsn√§her:** In der echten Welt variieren Startbedingungen auch
4. **RL-Best-Practice:** Variation f√∂rdert besseres Lernen

**Reproduzierbarkeit:** Durch Seed-Parameter bei `reset()` steuerbar

**Status:** ‚úÖ Implementiert in `envs/rllib_economy_env.py`

---

### Multi-Agent Setup

**Entscheidung:** Jeder Haushalt und jede Firma ist ein eigenst√§ndiger RL-Agent.

**Architektur:**
- **10 Haushalte** = 10 individuelle Agents (`household_0` bis `household_9`)
- **5 Firmen** = 5 individuelle Agents (`firm_0` bis `firm_4`)
- **Gesamt:** 15 Agents trainieren parallel

**Policy Sharing:**
- Alle Haushalte teilen sich **eine Policy** (`household_policy`)
- Alle Firmen teilen sich **eine Policy** (`firm_policy`)
- ‚Üí Nur 2 Policies f√ºr 15 Agents (effizient!)

**Begr√ºndung:**
- Parameter-Effizienz: Haushalte √§hneln sich, m√ºssen nicht individuell trainiert werden
- Schnelleres Lernen: Mehr Erfahrungen pro Policy-Update
- Skalierbar: Sp√§ter 100 Haushalte ohne neue Policy

**Status:** ‚úÖ Implementiert mit RLlib MultiAgentEnv

---

### Konfiguration via YAML

**Entscheidung:** Alle Agent-Parameter in separater Config-Datei.

**Struktur:**
```yaml
households:
  count: 10
  initial_cash:
    min: 1000    # Alle Haushalte ziehen aus diesem Bereich
    max: 5000

firms:
  count: 5
  initial_capital:
    min: 100000  # Alle Firmen ziehen aus diesem Bereich
    max: 500000
```

**Begr√ºndung:**
- Einfache Anpassung ohne Code-√Ñnderungen
- Dokumentation der Parameter
- Verschiedene Configs f√ºr Experimente m√∂glich
- Standard in ML-Projekten

**Status:** ‚úÖ Implementiert in `configs/agent_config.yaml`

---

### Wirtschaftslogik: Erst simpel, dann realistisch

**Entscheidung Phase 1:** Minimale Wirtschaftslogik f√ºr erste Tests.

**Aktuell implementiert:**
- Haushalte: W√§hlen Konsumquote (0-100% vom Cash)
- Firmen: Produzieren, setzen Preise, stellen ein/entlassen
- Bankrott-Mechanismus: Cash < 0 ‚Üí Agent bankrott

**Noch NICHT implementiert (kommt sp√§ter):**
- ‚ùå Arbeitsmarkt (Haushalte erhalten Lohn)
- ‚ùå G√ºtermarkt (Produktion ‚Üí Verkauf ‚Üí Konsum)
- ‚ùå Preismechanismus (Angebot/Nachfrage)
- ‚ùå Geldfluss-Kreislauf (Lohn ‚Üí Konsum ‚Üí Revenue)

**Begr√ºndung:**
1. **Erst Multi-Agent zum Laufen bringen** ‚Üí Dann verfeinern
2. Simple Version ist leichter zu debuggen
3. Iterative Entwicklung: Komplexe Features schrittweise hinzuf√ºgen

**N√§chster Schritt:** Nach erfolgreichem ersten Training realistischere Wirtschaftslogik einbauen

**Status:** üü° Phase 1 (simpel) implementiert, Phase 2 (realistisch) geplant

---

## Was wurde implementiert

### Version 0.2 - Multi-Agent RLlib Integration (28.01.2026)

**Erstellt:**
1. `envs/rllib_economy_env.py` - RLlib-kompatibler Multi-Agent Wrapper
2. `train/train_local.py` - PPO Training-Script
3. `train/quick_test.py` - Test-Script f√ºr Multi-Agent Setup
4. `train/README.md` - Training-Dokumentation

**Features:**
- ‚úÖ 15 individuelle Agents (10 Haushalte + 5 Firmen)
- ‚úÖ Policy Sharing (2 Policies f√ºr 15 Agents)
- ‚úÖ Separate Action/Observation Spaces
- ‚úÖ PPO-Algorithmus konfiguriert
- ‚úÖ Checkpoint-System
- ‚úÖ Command-Line Interface

**Testing:**
```bash
# Quick Test (funktioniert!)
python train/quick_test.py

# Volles Training
python train/train_local.py --timesteps 10000
```

**Status:** üü¢ **Ready for Training!**

---

### Version 0.1 - Basis-Setup (28.01.2026)

**Erstellt:**
1. `configs/agent_config.yaml` - Konfiguration mit Min/Max-Bereichen
2. `envs/simple_economy_env.py` - Simples Environment (Gymnasium-kompatibel)
3. `tests/test_simple_env.py` - Basis-Tests

**Features:**
- ‚úÖ 10 Haushalte mit Cash aus [1000-5000‚Ç¨]
- ‚úÖ 5 Firmen mit Kapital aus [100k-500k‚Ç¨]
- ‚úÖ Bankrott-Mechanismus
- ‚úÖ 250 Tage pro Jahr, 5 Jahre Training

---

## Technische Details

### Zeitstruktur

- **1 Step** = 1 Betriebstag
- **1 Episode** = 250 Tage = 1 Wirtschaftsjahr
- **Training** = 5 Jahre = 1250 Episoden = 312.500 Steps

### Action Spaces

**Haushalte:**
```python
Box(low=[0.0], high=[1.0])  # Konsumquote (0-100%)
```

**Firmen:**
```python
Box(
    low=[0.0, 5.0, -2.0],   # [Produktion, Preis, Mitarbeiter√§nderung]
    high=[200.0, 15.0, 2.0]
)
```

### Observation Spaces

**Haushalte:**
```python
Box(
    low=[0.0, 0.0, 0.0],     # [Cash, Durchschnittspreis, Besch√§ftigt]
    high=[100000.0, 50.0, 1.0]
)
```

**Firmen:**
```python
Box(
    low=[0.0, 0.0, 0.0, 0.0],      # [Kapital, Lager, Mitarbeiter, Nachfrage]
    high=[1000000.0, 1000.0, 50.0, 1000.0]
)
```

### Reward-Funktionen (simpel)

**Haushalte:**
```python
if bankrupt:
    reward = -10.0
else:
    reward = consumption * 0.1 + 1.0  # Konsum + √úberleben
```

**Firmen:**
```python
if bankrupt:
    reward = -10.0
else:
    reward = capital / 100000.0  # Kapital normalisiert
```

---

## N√§chste Schritte

### Phase 1: Erstes Training (üî¥ **JETZT**)

1. ‚úÖ Quick Test durchf√ºhren
2. üî¥ Kurzes Training (10k steps) starten
3. üî¥ Metriken analysieren: Lernen die Agents?
4. üî¥ DOCUMENTATION.md updaten mit Ergebnissen

### Phase 2: Wirtschaftslogik verbessern

1. Arbeitsmarkt implementieren (Firmen ‚Üí L√∂hne ‚Üí Haushalte)
2. G√ºtermarkt implementieren (Produktion ‚Üí Verkauf ‚Üí Konsum)
3. Geldfluss-Kreislauf schlie√üen
4. Reward-Funktionen anpassen

### Phase 3: Cloud Deployment

1. Backend (FastAPI) f√ºr Inference
2. Frontend (Streamlit) f√ºr Visualisierung
3. Google Cloud Platform Setup
4. CI/CD Pipeline

---

## Lessons Learned

### Was funktioniert gut
- ‚úÖ RLlib Multi-Agent API ist sehr elegant
- ‚úÖ Policy Sharing spart massiv Parameter
- ‚úÖ YAML-Config macht Experimente einfach
- ‚úÖ Quick-Test-Script verhindert lange Debug-Sessions

### Was noch offen ist
- ‚ùì Lernen die Agents sinnvoll mit simpler Wirtschaftslogik?
- ‚ùì Wie schnell konvergiert das Training?
- ‚ùì Brauchen wir komplexere Rewards?

---

## Git-History

### Commits (neueste zuerst)

- `09d5ecc` - docs: Add training README with quick test instructions
- `ad68eb9` - feat: Add quick test script for multi-agent setup
- `fb9079a` - feat: Add RLlib multi-agent training setup
- `617edc9` - feat: Add simple multi-agent economy environment
- `38f78bd` - feat: Add simple agent configuration

### Branches
- `main` - Hauptentwicklung

### Tags
(Kommen nach ersten erfolgreichen Trainings)

---

## Team-Notizen

**28.01.2026 - 12:57 Uhr:**
- Multi-Agent Setup vollst√§ndig implementiert
- Quick-Test-Script hinzugef√ºgt
- Bereit f√ºr erstes Training!
- N√§chster Schritt: Training durchf√ºhren und Ergebnisse dokumentieren

---

**Letztes Update:** 28.01.2026, 12:58 Uhr
