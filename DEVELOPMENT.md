# ğŸ“ VWL-RL-Cloud: Entwicklungs-Dokumentation

**Projekt:** Volkswirtschafts-Simulation mit Multi-Agent RL  
**Modul:** Fortgeschrittene KI-Anwendungen & Cloud & Big Data  
**Zeitraum:** Januar 2026  
**Autor:** H3nri5H (Foxyy)

---

## ğŸ¯ Projekt-Ziel

**Vision:**  
Multi-Agent Reinforcement Learning System zur Simulation makroÃ¶konomischer Prozesse. Drei Agent-Typen (Firmen, Haushalte, Staat) lernen eigenstÃ¤ndig optimale Strategien in einer simulierten Volkswirtschaft.

**Module-Anforderungen:**
- âœ… **KI-Anwendungen**: Multi-Agent RL mit Ray RLlib
- âœ… **Cloud**: Zustandslose (Frontend) + Zustandsbehaftete (Backend) Komponenten auf GCP

---

## ğŸ“… Entwicklungs-Timeline

### **Phase 1: Setup & Grundstruktur** (21.01.2026)

#### âœ… **Morgens: Repository & Environment**
- Repository erstellt: `github.com/H3nri5H/VWL-RL-Cloud`
- Basis-Environment implementiert (`envs/economy_env.py`)
- Gymnasium-kompatible Struktur
- Firmen + Haushalte als Dictionaries
- Regierung als RL-Agent (Single-Agent)

**Technische Entscheidungen:**
- Python 3.11 (beste Ray-KompatibilitÃ¤t)
- Ray RLlib 2.10 (stabiler als 2.x+)
- Gymnasium 0.28.1 (Ray-Requirement)
- PPO-Algorithmus (stabil fÃ¼r continuous actions)

---

#### âœ… **Mittags: Setup-Automatisierung**

**Problem:** Dependency-Konflikte bei Installation
- Ray 2.10 braucht Gymnasium 0.28.1 (nicht 0.29.1)
- PYTHONPATH-Probleme bei Tests
- pytest-Dependency unnÃ¶tig

**LÃ¶sung:**
- `setup.py` erstellt (automatisches Setup)
- `setup.bat` erstellt (Windows One-Click)
- `requirements.txt` gefixt (korrekte Gymnasium-Version)
- Tests ohne pytest neu geschrieben (PYTHONPATH auto-fix)
- README komplett Ã¼berarbeitet (idiotensicher)

**Ergebnis:** Setup in 10 Minuten fÃ¼r komplette AnfÃ¤nger mÃ¶glich

---

#### âœ… **Nachmittags: Streamlit Frontend**
- Interaktive Web-UI implementiert
- 3 Parameter-Sliders (Steuern, Ausgaben, Zinsen)
- 4 Szenarien (Normal, Rezession, Boom, Inflation)
- Live-Plots mit Plotly (BIP, Inflation, Arbeitslosigkeit)
- 100-Step Simulation pro Klick

**Architektur-Typ:** Zustandslos (Environment wird bei jedem Request neu erstellt)

---

#### âœ… **Abends: Zeitstruktur & Training** (18:00 - 18:50 Uhr)

**Diskussion: Multi-Agent Architektur**

**Problem identifiziert:**  
"Moving Target Problem" - Agents beeinflussen sich gegenseitig, Environment wird non-stationary

**LÃ¶sungsansÃ¤tze diskutiert:**
1. CTDE (Centralized Training, Decentralized Execution) - MAPPO
2. Self-Play (wie AlphaGo)
3. Population-Based Training
4. Curriculum Learning (stufenweises Trainieren)

**Entscheidung:**  
- START: Single-Agent (nur Staat) mit regelbasierten Firmen/Haushalten
- SPÃ„TER: Multi-Agent wenn Zeit bleibt
- GRUND: Stabiler, weniger Risiko, fÃ¼r PrÃ¤sentation ausreichend

**Implementiert:**
- âœ… Zeitstruktur: 1 Step = 1 Tag, 1 Episode = 365 Tage = 1 Jahr
- âœ… `current_day` & `current_year` Tracking
- âœ… Jahresabschluss-Logik (Metriken sammeln)
- âœ… Episode endet nach 5 Jahren (konfigurierbar)
- âœ… DEVELOPMENT.md fÃ¼r Dokumentation angelegt

**Training-Bugs behoben:**
1. âœ… EnvContext vs. Dict Problem (max_years Parameter)
2. âœ… Observation Space Bounds (debt/BIP kÃ¶nnen negativ sein)
3. âœ… Ray 2.10 API (rollouts statt env_runners)
4. âœ… PYTHONPATH auto-fix in allen Scripts
5. âœ… Checkpoint-Handling (TrainingResult Objekt)

**Training Features:**
- âœ… Clean Output (keine verbose Logs)
- âœ… Strg+C sicheres Beenden (speichert Model automatisch)
- âœ… Checkpoints alle 5 Iterationen
- âœ… Best-Model-Tracking

---

## ğŸ§  Architektur-Entscheidungen

### **Environment Design**

**Aktuell (Phase 1 - Single Agent):**
```
Regierung (RL-Agent)
  â†“ Actions: [tax_rate, gov_spending, interest_rate]
  â†“
Wirtschaft (Simulation)
  â”œâ”€ 10 Firmen (regelbasiert)
  â”œâ”€ 50 Haushalte (regelbasiert) 
  â””â”€ Markt (Clearing)
  â†“
Observations: [bip, inflation, unemployment, debt, interest]
Reward: f(bip_growth, unemployment, inflation, deficit)
```

**Geplant (Phase 2 - Multi Agent):**
```
Firmen (10x RL-Agents)
  Actions: [price, wage, hire/fire, investment]
  Reward: profit

Haushalte (50x RL-Agents)
  Actions: [consumption_rate, savings_rate, job_search]
  Reward: consumption + savings - unemployment_penalty

Regierung (1x RL-Agent)
  Actions: [tax, spending, interest, unemployment_aid]
  Reward: gdp_growth - unemployment - abs(inflation-2%)
```

---

### **Reward-Funktion (Staat)**

**Aktuell:**
```python
reward = (
    + bip_growth * 10        # BIP-Wachstum belohnen
    - unemployment * 20       # Arbeitslosigkeit stark bestrafen
    - abs(inflation) * 15     # Inflation (egal ob +/-) bestrafen
    - abs(deficit) * 0.01     # Defizit leicht bestrafen
)
```

**Rationale:**
- BIP-Wachstum = primÃ¤res Ziel (Wohlstand)
- Arbeitslosigkeit = soziales Problem (hohe Strafe)
- Inflation = StabilitÃ¤t (symmetrisch bestraft)
- Defizit = nachhaltig (kleine Strafe, nicht primÃ¤r)

**Balancing:** Wird wÃ¤hrend Training angepasst falls nÃ¶tig

---

### **Zeitstruktur**

**Mapping:**
- 1 Step = 1 Tag
- 365 Steps = 1 Jahr = 1 Episode
- Training Ã¼ber mehrere Episoden = mehrere Jahre

**Vorteile:**
- Realistische Zeitskalen
- Saisonale Effekte mÃ¶glich (spÃ¤ter)
- JahresabschlÃ¼sse fÃ¼r Metriken
- Vergleichbar mit realen Daten

**Episode-Terminierung:**
- Nach 5 Jahren (1825 Steps) - Trainingsdefault
- Konfigurierbar fÃ¼r lÃ¤ngere Simulationen

---

## ğŸ‹ï¸ Training Guide

### **Quick Start**
```bash
python train/train_single.py
```

### **Ãœber-Nacht-Training**

**In `train/train_single.py` Ã¤ndern:**
```python
NUM_ITERATIONS = 200    # Statt 20
MAX_YEARS = 10          # LÃ¤ngere Episoden
```

**Dann starten:**
```bash
python train/train_single.py
# Laptop NICHT zuklappen!
# Windows: Energieeinstellungen â†’ "Niemals in Ruhezustand"
```

**Sicheres Beenden:**
- `Strg+C` drÃ¼cken
- Script speichert Model automatisch
- Letzter Checkpoint bleibt erhalten

**GeschÃ¤tzte Dauern:**
- 20 Iterationen @ 5 Jahre: ~5 Minuten
- 100 Iterationen @ 10 Jahre: ~45 Minuten
- 200 Iterationen @ 10 Jahre: ~1.5 Stunden

---

## ğŸ› Bekannte Probleme & LÃ¶sungen

### **Problem 1: Gymnasium Version Conflict**
**Symptom:** `ResolutionImpossible` bei pip install  
**Ursache:** Ray 2.10 braucht exakt Gymnasium 0.28.1  
**LÃ¶sung:** requirements.txt auf 0.28.1 gefixt  
**Status:** âœ… GelÃ¶st

### **Problem 2: ModuleNotFoundError 'envs'**
**Symptom:** Import Error bei Tests  
**Ursache:** PYTHONPATH nicht gesetzt  
**LÃ¶sung:** Auto-fix in allen Scripts (`sys.path.insert`)  
**Status:** âœ… GelÃ¶st

### **Problem 3: Non-Stationary Environment (Multi-Agent)**
**Symptom:** Agents' Strategien werden stÃ¤ndig invalidiert  
**Ursache:** Gegenseitige Beeinflussung der Agents  
**LÃ¶sung:** Start mit Single-Agent, spÃ¤ter MAPPO/Self-Play  
**Status:** ğŸŸ¡ Design-Entscheidung getroffen

### **Problem 4: Ray Deprecation Warnings**
**Symptom:** Viele Warnings bei Training  
**Ursache:** Ray 2.10 ist alt (2.7+ Warnings)  
**LÃ¶sung:** `os.environ['PYTHONWARNINGS'] = 'ignore'` am Script-Anfang  
**Status:** âœ… GelÃ¶st

### **Problem 5: Observation Space Out of Bounds**
**Symptom:** `ValueError` bei negativer Debt  
**Ursache:** Observation Space erlaubte nur positive Werte  
**LÃ¶sung:** Bounds auf `-10 bis +10` erweitert + Clipping  
**Status:** âœ… GelÃ¶st

---

## ğŸ“ TODO / NÃ¤chste Schritte

### **Sofort (Diese Woche)**
- [x] Training mit Zeitstruktur funktioniert
- [x] Strg+C sicheres Beenden
- [x] Clean Output (keine Warnings)
- [ ] LÃ¤ngeres Training (100+ Iterationen)
- [ ] Reward-Kurve analysieren
- [ ] Hyperparameter tunen falls nÃ¶tig

### **NÃ¤chste Woche**
- [ ] FastAPI Backend implementieren (Model Loading)
- [ ] Frontend: Trainiertes Model laden
- [ ] Docker Images bauen
- [ ] GCP Account einrichten

### **Falls Zeit bleibt (Optional)**
- [ ] Multi-Agent: Firmen als Agents
- [ ] TensorBoard Logging
- [ ] Historische Daten integrieren

---

## ğŸ“Š Projekt-Fortschritt

**Phase 1: Grundlagen** (âœ… 100%)  
- Setup & Dependencies  
- Environment Implementierung  
- Tests  
- Frontend  
- Zeitstruktur  

**Phase 2: Training** (ğŸ”„ 90%)  
- Single-Agent Training funktioniert
- Clean Output implementiert
- Strg+C Handler fertig
- Noch offen: LÃ¤ngeres Training

**Phase 3: Cloud** (â³ 0%)  
- Backend API  
- Containerization  
- GCP Deployment  

**Phase 4: Multi-Agent** (â³ 0%)  
- Firmen-Agents  
- MAPPO Training  

---

## ğŸ“ Notizen & Erkenntnisse

### **21.01.2026 18:00 - Zeitstruktur**
- 1 Tag = 1 Step macht Simulation intuitiver
- ErmÃ¶glicht spÃ¤ter saisonale Effekte
- JahresabschlÃ¼sse fÃ¼r Metriken-Aggregation
- 5 Jahre Default = guter Kompromiss

### **21.01.2026 18:30 - Training-Bugs**
- EnvContext vs. Dict Problem gelÃ¶st (flexible Extraktion)
- Observation Space Bounds wichtig (negative Werte!)
- Ray 2.10 API anders als 2.x+ (rollouts nicht env_runners)

### **21.01.2026 18:50 - Clean Training**
- Strg+C Handler kritisch fÃ¼r Ãœber-Nacht-Training
- Warnings unterdrÃ¼cken verbessert UX massiv
- Best-Model-Tracking automatisch
- Checkpoints alle 5 Iterationen = guter Kompromiss

---

**Letztes Update:** 21.01.2026, 18:50 Uhr  
**Status:** ğŸŸ¢ Active Development  
**NÃ¤chster Meilenstein:** LÃ¤ngeres Training (100+ Iter)
