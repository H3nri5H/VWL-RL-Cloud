# ğŸ“ VWL-RL-Cloud: Entwicklungs-Dokumentation

**Projekt:** Volkswirtschafts-Simulation mit Multi-Agent RL  
**Modul:** Fortgeschrittene KI-Anwendungen & Cloud & Big Data  
**Zeitraum:** Januar 2026  
**Autor:** H3nri5H (Foxyy)

---

## ğŸ¯ Projekt-Ziel

**Vision:**  
Multi-Agent Reinforcement Learning System zur Simulation makroÃ¶konomischer Prozesse. Drei Agent-Typen (Firmen, Haushalte, Staat) lernen eigenstÃ¤ndig optimale Strategien in einer simulierten Volkswirtschaft.

**Module-Anforderungen:**
- âœ… **KI-Anwendungen**: Multi-Agent RL mit Ray RLlib
- âœ… **Cloud**: Zustandslose (Frontend) + Zustandsbehaftete (Backend) Komponenten auf GCP

---

## ğŸ“… Entwicklungs-Timeline

### **Phase 1: Setup & Grundstruktur** (21.01.2026)

#### âœ… **Morgens: Repository & Environment**
- Repository erstellt: `github.com/H3nri5H/VWL-RL-Cloud`
- Basis-Environment implementiert (`envs/economy_env.py`)
- Gymnasium-kompatible Struktur
- Firmen + Haushalte als Dictionaries
- Regierung als RL-Agent (Single-Agent)

**Technische Entscheidungen:**
- Python 3.11 (beste Ray-KompatibilitÃ¤t)
- Ray RLlib 2.10 (stabiler als 2.x+)
- Gymnasium 0.28.1 (Ray-Requirement)
- PPO-Algorithmus (stabil fÃ¼r continuous actions)

---

#### âœ… **Mittags: Setup-Automatisierung**

**Problem:** Dependency-Konflikte bei Installation
- Ray 2.10 braucht Gymnasium 0.28.1 (nicht 0.29.1)
- PYTHONPATH-Probleme bei Tests
- pytest-Dependency unnÃ¶tig

**LÃ¶sung:**
- `setup.py` erstellt (automatisches Setup)
- `setup.bat` erstellt (Windows One-Click)
- `requirements.txt` gefixt (korrekte Gymnasium-Version)
- Tests ohne pytest neu geschrieben (PYTHONPATH auto-fix)
- README komplett Ã¼berarbeitet (idiotensicher)

**Ergebnis:** Setup in 10 Minuten fÃ¼r komplette AnfÃ¤nger mÃ¶glich

---

#### âœ… **Nachmittags: Streamlit Frontend**
- Interaktive Web-UI implementiert
- 3 Parameter-Sliders (Steuern, Ausgaben, Zinsen)
- 4 Szenarien (Normal, Rezession, Boom, Inflation)
- Live-Plots mit Plotly (BIP, Inflation, Arbeitslosigkeit)
- 100-Step Simulation pro Klick

**Architektur-Typ:** Zustandslos (Environment wird bei jedem Request neu erstellt)

---

#### âœ… **Abends: Zeitstruktur & Dokumentation** (18:00 - 18:10 Uhr)

**Diskussion: Multi-Agent Architektur**

**Problem identifiziert:**  
"Moving Target Problem" - Agents beeinflussen sich gegenseitig, Environment wird non-stationary

**LÃ¶sungsansÃ¤tze diskutiert:**
1. CTDE (Centralized Training, Decentralized Execution) - MAPPO
2. Self-Play (wie AlphaGo)
3. Population-Based Training
4. Curriculum Learning (stufenweises Trainieren)

**Entscheidung:**  
- START: Single-Agent (nur Staat) mit regelbasierten Firmen/Haushalten
- SPÃ„TER: Multi-Agent wenn Zeit bleibt
- GRUND: Stabiler, weniger Risiko, fÃ¼r PrÃ¤sentation ausreichend

**Implementiert:**
- âœ… Zeitstruktur: 1 Step = 1 Tag, 1 Episode = 365 Tage = 1 Jahr
- âœ… `current_day` & `current_year` Tracking
- âœ… Jahresabschluss-Logik (Metriken sammeln)
- âœ… Episode endet nach 5 Jahren (trainierbar auf 10-50 Jahre)
- âœ… DEVELOPMENT.md fÃ¼r Dokumentation angelegt

---

## ğŸ§  Architektur-Entscheidungen

### **Environment Design**

**Aktuell (Phase 1 - Single Agent):**
```
Regierung (RL-Agent)
  â†“ Actions: [tax_rate, gov_spending, interest_rate]
  â†“
Wirtschaft (Simulation)
  â”œâ”€ 10 Firmen (regelbasiert)
  â”œâ”€ 50 Haushalte (regelbasiert) 
  â””â”€ Markt (Clearing)
  â†“
Observations: [bip, inflation, unemployment, debt, interest]
Reward: f(bip_growth, unemployment, inflation, deficit)
```

**Geplant (Phase 2 - Multi Agent):**
```
Firmen (10x RL-Agents)
  Actions: [price, wage, hire/fire, investment]
  Reward: profit

Haushalte (50x RL-Agents)
  Actions: [consumption_rate, savings_rate, job_search]
  Reward: consumption + savings - unemployment_penalty

Regierung (1x RL-Agent)
  Actions: [tax, spending, interest, unemployment_aid]
  Reward: gdp_growth - unemployment - abs(inflation-2%)
```

---

### **Reward-Funktion (Staat)**

**Aktuell:**
```python
reward = (
    + bip_growth * 10        # BIP-Wachstum belohnen
    - unemployment * 20       # Arbeitslosigkeit stark bestrafen
    - abs(inflation) * 15     # Inflation (egal ob +/-) bestrafen
    - abs(deficit) * 0.01     # Defizit leicht bestrafen
)
```

**Rationale:**
- BIP-Wachstum = primÃ¤res Ziel (Wohlstand)
- Arbeitslosigkeit = soziales Problem (hohe Strafe)
- Inflation = StabilitÃ¤t (symmetrisch bestraft)
- Defizit = nachhaltig (kleine Strafe, nicht primÃ¤r)

**Balancing:** Wird wÃ¤hrend Training angepasst falls nÃ¶tig

---

### **Zeitstruktur**

**Mapping:**
- 1 Step = 1 Tag
- 365 Steps = 1 Jahr = 1 Episode
- Training Ã¼ber mehrere Episoden = mehrere Jahre

**Vorteile:**
- Realistische Zeitskalen
- Saisonale Effekte mÃ¶glich (spÃ¤ter)
- JahresabschlÃ¼sse fÃ¼r Metriken
- Vergleichbar mit realen Daten

**Episode-Terminierung:**
- Nach 5 Jahren (1825 Steps) - Trainingsdefault
- Konfigurierbar fÃ¼r lÃ¤ngere Simulationen

---

## ğŸ› Bekannte Probleme & LÃ¶sungen

### **Problem 1: Gymnasium Version Conflict**
**Symptom:** `ResolutionImpossible` bei pip install  
**Ursache:** Ray 2.10 braucht exakt Gymnasium 0.28.1  
**LÃ¶sung:** requirements.txt auf 0.28.1 gefixt  
**Status:** âœ… GelÃ¶st

### **Problem 2: ModuleNotFoundError 'envs'**
**Symptom:** Import Error bei Tests  
**Ursache:** PYTHONPATH nicht gesetzt  
**LÃ¶sung:** Auto-fix in test_env.py (`sys.path.insert`)  
**Status:** âœ… GelÃ¶st

### **Problem 3: Non-Stationary Environment (Multi-Agent)**
**Symptom:** Agents' Strategien werden stÃ¤ndig invalidiert  
**Ursache:** Gegenseitige Beeinflussung der Agents  
**LÃ¶sung:** Start mit Single-Agent, spÃ¤ter MAPPO/Self-Play  
**Status:** ğŸŸ¡ Design-Entscheidung getroffen

---

## ğŸ“ TODO / NÃ¤chste Schritte

### **Sofort (Diese Woche)**
- [ ] Training mit neuer Zeitstruktur testen
- [ ] Hyperparameter tunen (Learning Rate, Batch Size)
- [ ] LÃ¤ngeres Training (10+ Jahre)
- [ ] Reward-Funktion evaluieren & ggf. anpassen

### **NÃ¤chste Woche**
- [ ] FastAPI Backend implementieren (Model Loading)
- [ ] TensorBoard Logging aktivieren
- [ ] Docker Images bauen
- [ ] GCP Account einrichten

---

**Letztes Update:** 21.01.2026, 18:10 Uhr  
**Status:** ğŸŸ¢ Active Development  
**NÃ¤chster Meilenstein:** Training mit Zeitstruktur
