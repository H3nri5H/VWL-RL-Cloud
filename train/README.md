# Training Scripts

Dieses Verzeichnis enthÃ¤lt alle Training-Scripte fÃ¼r die Multi-Agent Wirtschafts-Simulation.

---

## ğŸš€ Quick Start: Multi-Agent Setup testen

### 1. Installation

```bash
pip install -r requirements.txt
```

### 2. Quick Test (empfohlen vor erstem Training)

```bash
python train/quick_test.py
```

**Was wird getestet:**
- âœ… Environment initialisiert korrekt
- âœ… Alle 15 Agents vorhanden (10 Haushalte + 5 Firmen)
- âœ… Actions funktionieren
- âœ… Mini-Training lÃ¤uft (100 steps)

**Output (sollte so aussehen):**
```
============================================================
ğŸ§ª QUICK TEST: Multi-Agent Economy
============================================================

[1/4] Environment initialisieren...
      âœ… 15 Agents gefunden
         - Haushalte: 10
         - Firmen: 5

[2/4] Environment Reset...
      âœ… Observations: 15 agents

[3/4] Step mit Random Actions...
      âœ… Step erfolgreich

[4/4] Mini-Training starten (100 steps)...
      Training lÃ¤uft...
      âœ… Training erfolgreich!

============================================================
âœ… ALLE TESTS BESTANDEN!
============================================================
```

---

## ğŸ‹ï¸ Volles Training

### Lokales Training starten

```bash
# Kurzes Test-Training (10k steps â‰ˆ 5 Minuten)
python train/train_local.py --timesteps 10000

# Mittleres Training (100k steps â‰ˆ 1 Stunde)
python train/train_local.py --timesteps 100000

# Volles Training (1.25M steps = 5 Jahre â‰ˆ 6-8 Stunden)
python train/train_local.py --timesteps 1250000
```

### Parameter

```bash
python train/train_local.py \
  --timesteps 100000 \           # Anzahl Training-Steps
  --checkpoint-freq 10 \         # Alle 10 Iterationen Checkpoint
  --output-dir ./ray_results \  # Output-Verzeichnis
  --num-workers 2 \              # Anzahl Worker
  --num-gpus 0                   # Anzahl GPUs (0 = CPU)
```

### Training-Output

WÃ¤hrend des Trainings siehst du:

```
+-------------------------+------------+
| Trial name              | status     |
+-------------------------+------------+
| PPO_economy_xxx         | RUNNING    |
+-------------------------+------------+
| episode_reward_mean     | 5.23       |
| episodes_this_iter      | 8          |
| timesteps_total         | 2000       |
+-------------------------+------------+
```

**Wichtige Metriken:**
- `episode_reward_mean`: Durchschnittlicher Reward (sollte steigen!)
- `episodes_this_iter`: Anzahl abgeschlossene Episoden
- `timesteps_total`: Fortschritt im Training

---

## ğŸ’¾ Checkpoints

Models werden automatisch gespeichert in:

```
ray_results/
â””â”€â”€ economy_training/
    â””â”€â”€ PPO_economy_xxx/
        â”œâ”€â”€ checkpoint_000010/  # Nach 10 Iterationen
        â”œâ”€â”€ checkpoint_000020/
        â””â”€â”€ checkpoint_000030/
```

### Checkpoint laden und weitertrainieren

```python
from ray.rllib.algorithms.ppo import PPO

algo = PPO.from_checkpoint("ray_results/economy_training/PPO_xxx/checkpoint_000010")
result = algo.train()  # Weitertrainieren
```

---

## ğŸ Troubleshooting

### Problem: `ModuleNotFoundError: No module named 'ray'`

```bash
pip install ray[rllib]==2.9.0 torch==2.1.0
```

### Problem: `FileNotFoundError: configs/agent_config.yaml not found`

```bash
# Script muss aus Root-Verzeichnis ausgefÃ¼hrt werden!
cd VWL-RL-Cloud
python train/train_local.py
```

### Problem: Training ist sehr langsam

```bash
# Mehr Worker nutzen (max = CPU-Kerne - 1)
python train/train_local.py --num-workers 4

# Oder kleinere Batch-Size (in train_local.py Ã¤ndern)
```

### Problem: `OutOfMemoryError`

```bash
# Weniger Worker
python train/train_local.py --num-workers 1

# Oder kleinere Batch-Size in train_local.py:
train_batch_size=2000  # statt 4000
```

---

## ğŸ“Š TensorBoard

Training visualisieren (optional):

```bash
tensorboard --logdir=ray_results/economy_training
```

Dann im Browser: `http://localhost:6006`

---

## ğŸ“‹ NÃ¤chste Schritte nach erfolgreichem Training

1. **Evaluation:** Model testen mit Evaluations-Script (kommt noch)
2. **Versionierung:** Bestes Model als `v1.0` taggen
3. **Dokumentation:** `DOCUMENTATION.md` updaten mit Ergebnissen
4. **Cloud:** Training auf GCP laufen lassen (spÃ¤ter)

---

## ğŸ“ Logs

Alle Logs findest du in:

```
ray_results/economy_training/PPO_xxx/
â”œâ”€â”€ progress.csv          # Training-Metriken als CSV
â”œâ”€â”€ result.json          # Detaillierte Results
â””â”€â”€ events.out.tfevents  # TensorBoard-Logs
```

---

**Status:** ğŸŸ¢ Multi-Agent Setup funktioniert, Ready for Training!
