# VWL-RL-Cloud ğŸ­

**Multi-Agent Reinforcement Learning fÃ¼r Volkswirtschafts-Simulation**  
DHSH Module: Fortgeschrittene KI-Anwendungen & Cloud & Big Data | Januar 2026

---

## ğŸ’ Ãœberblick

Simulation einer Volkswirtschaft mit RL-Agents:
- **10 Haushalte** (konsumieren, arbeiten)
- **5 Unternehmen** (produzieren, stellen ein, setzen Preise)
- **Kein Staat** (erstmal - fokus auf Basis-Interaktion)

Jeder Agent wird von einem eigenen RL-Model gesteuert.

---

## ğŸš€ Quick Start (Lokal)

### 1. Repository klonen

```bash
git clone https://github.com/H3nri5H/VWL-RL-Cloud.git
cd VWL-RL-Cloud
```

### 2. Dependencies installieren

```bash
# Python 3.11+ required
pip install -r requirements.txt
```

### 3. Environment testen

```bash
# Basis-Test
python tests/test_simple_env.py

# Oder direkt Environment starten
python envs/simple_economy_env.py
```

**Ausgabe sollte sein:**
```
âœ… Initiale Bedingungen erstellt (fix fÃ¼r alle Episoden):
   Haushalte: 10 mit Cash 1200â‚¬ - 4800â‚¬
   Firmen: 5 mit Kapital 120000â‚¬ - 480000â‚¬

ğŸ§ª Testing SimpleEconomyEnv...
âœ… Reset successful
...
```

### 4. Training (kommt spÃ¤ter)

```bash
# Lokal trainieren (wenn implementiert)
python train/train_local.py --version v1.0

# Model liegt dann in: models/v1.0.zip
```

---

## ğŸ“‹ Projekt-Struktur

```
VWL-RL-Cloud/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ agent_config.yaml         # âœ… Startbedingungen (Min/Max fÃ¼r alle)
â”‚
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ simple_economy_env.py     # âœ… Gymnasium Environment (Haushalte+Firmen)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_simple_env.py        # âœ… Test fÃ¼r fixe Startbedingungen
â”‚
â”œâ”€â”€ train/                       # âŒ TODO: Training Scripts
â”‚   â”œâ”€â”€ train_local.py
â”‚   â””â”€â”€ train_cloud.py
â”‚
â”œâ”€â”€ backend/                     # âŒ TODO: FastAPI (zustandsbehaftet)
â”‚   â”œâ”€â”€ serve.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/                    # âŒ TODO: Streamlit (zustandslos)
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ deploy/                      # âŒ TODO: Cloud Deployment
â”‚   â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ k8s/
â”‚
â”œâ”€â”€ models/                      # Models werden hier gespeichert
â”‚   â””â”€â”€ latest_model.zip         # FÃ¼r Dozenten (kommt spÃ¤ter)
â”‚
â”œâ”€â”€ DOCUMENTATION.md            # âœ… Was wurde gemacht + Warum
â””â”€â”€ README.md                   # Diese Datei (Setup-Anleitung)
```

---

## âš™ï¸ Konfiguration

### Agent-Parameter anpassen

**Datei:** `configs/agent_config.yaml`

```yaml
households:
  count: 10  # Anzahl Haushalte
  
  initial_cash:
    min: 1000    # Minimum Startkapital
    max: 5000    # Maximum Startkapital

firms:
  count: 5  # Anzahl Unternehmen
  
  initial_capital:
    min: 100000  # 100kâ‚¬
    max: 500000  # 500kâ‚¬
  
  initial_employees:
    min: 3
    max: 8

simulation:
  days_per_year: 250  # Betriebstage
  max_years: 5        # Training-Dauer
```

**Wichtig:** Diese Werte werden **einmal beim Init** gezogen und bleiben dann **Ã¼ber alle Episoden fix**!

---

## ğŸ§  Wie funktioniert das?

### Startbedingungen

```python
# Beim Training-Start (env.__init__):
env = SimpleEconomyEnv()

# Zieht fÃ¼r jeden Agent zufÃ¤llige Werte:
Haushalt_0: 2500â‚¬  (aus [1000-5000â‚¬])
Haushalt_1: 4200â‚¬  (aus [1000-5000â‚¬])
Firma_0: 250.000â‚¬  (aus [100k-500kâ‚¬])

# Diese Werte bleiben FIX!
```

### Episoden

```python
# Episode 1
obs = env.reset()  # Haushalte/Firmen bei Startwerten
for day in range(250):  # 1 Jahr
    action = agent.predict(obs)
    obs, reward, done, info = env.step(action)

# Episode 2
obs = env.reset()  # WIEDER bei Startwerten (NICHT weiterfÃ¼hren!)
# Haushalt_0 startet wieder mit 2500â‚¬
```

**Wichtig:** 
- Gewinn aus Episode 1 wird **NICHT** in Episode 2 Ã¼bernommen
- Jede Episode startet "frisch" mit den fixen Startwerten
- Aber: RL-Agent **lernt** aus allen Episoden!

---

## ğŸ“š Module-Anforderungen

### Fortgeschrittene KI-Anwendungen
- âœ… Multi-Agent Reinforcement Learning
- âœ… Custom Gymnasium Environment
- âŒ RL-Training (TODO)
- âŒ Reward-Design (TODO)

### Cloud & Big Data
- âŒ Zustandslose Komponente (Frontend)
- âŒ Zustandsbehaftete Komponente (Backend mit Models)
- âŒ Cloud Deployment (GCP)
- âŒ CI/CD Pipeline

---

## ğŸ› ï¸ Development

### Tests ausfÃ¼hren

```bash
python tests/test_simple_env.py
```

**PrÃ¼ft:**
- âœ… Startbedingungen bleiben Ã¼ber Episoden fix
- âœ… Environment kann resetten
- âœ… Steps funktionieren

### Environment direkt nutzen

```python
from envs.simple_economy_env import SimpleEconomyEnv

env = SimpleEconomyEnv()
obs, info = env.reset()

# Manuelle Aktionen
for _ in range(10):
    action = env.action_space.sample()  # ZufÃ¤llige Action
    obs, reward, done, info = env.step(action)
    print(f"Day {info['day']}: Reward={reward}")
```

---

## ğŸ“ Dokumentation

- **[DOCUMENTATION.md](DOCUMENTATION.md)** - Was wurde gemacht + Design-Entscheidungen
- **[configs/agent_config.yaml](configs/agent_config.yaml)** - Parameter-Dokumentation

---

## ğŸ‘¥ Team

**H3nri5H** (Foxyy)  
DHSH - Januar 2026

---

## ğŸ“Œ Status

**Version:** 0.1 - Basis-Setup  
**Stand:** 28.01.2026

**Implementiert:**
- âœ… Config mit Min/Max-Bereichen
- âœ… Simple Environment (Haushalte + Firmen)
- âœ… Fixe Startbedingungen
- âœ… Tests

**NÃ¤chste Schritte:**
1. Wirtschafts-Logik implementieren (Produktion, Konsum, Markt)
2. Action/Observation Spaces definieren
3. Reward-Funktionen designen
4. Lokales Training testen
5. Backend/Frontend implementieren
6. Cloud Deployment

---

**FÃ¼r Dozenten:** Ein trainiertes Model wird spÃ¤ter in `models/latest_model.zip` hochgeladen, sodass kein Training notwendig ist.
